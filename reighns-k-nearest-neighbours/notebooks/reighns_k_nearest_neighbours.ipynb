{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Topics\" data-toc-modified-id=\"Topics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Topics</a></span></li><li><span><a href=\"#Notations-and-Definitions\" data-toc-modified-id=\"Notations-and-Definitions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Notations and Definitions</a></span></li><li><span><a href=\"#Assumptions\" data-toc-modified-id=\"Assumptions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Assumptions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Normalization</a></span></li></ul></li><li><span><a href=\"#Hypothesis-Space-and-the-Learning-Algorithm\" data-toc-modified-id=\"Hypothesis-Space-and-the-Learning-Algorithm-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Hypothesis Space and the Learning Algorithm</a></span></li><li><span><a href=\"#K-Nearest-Neighbours-Algorithm\" data-toc-modified-id=\"K-Nearest-Neighbours-Algorithm-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>K-Nearest-Neighbours Algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Majority-Voting-Class-Label\" data-toc-modified-id=\"Majority-Voting-Class-Label-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Majority Voting Class Label</a></span></li><li><span><a href=\"#Probabilistic-Class-Label\" data-toc-modified-id=\"Probabilistic-Class-Label-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Probabilistic Class Label</a></span></li></ul></li><li><span><a href=\"#Regression\" data-toc-modified-id=\"Regression-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Majority-Mean-Class-Label\" data-toc-modified-id=\"Majority-Mean-Class-Label-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Majority Mean Class Label</a></span></li></ul></li><li><span><a href=\"#Additional-Implementations\" data-toc-modified-id=\"Additional-Implementations-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Additional Implementations</a></span><ul class=\"toc-item\"><li><span><a href=\"#KD-Tree\" data-toc-modified-id=\"KD-Tree-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>KD-Tree</a></span></li></ul></li></ul></li><li><span><a href=\"#Time-and-Space-Complexity\" data-toc-modified-id=\"Time-and-Space-Complexity-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Time and Space Complexity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-KNN-(see-knn_naive.py)\" data-toc-modified-id=\"Naive-KNN-(see-knn_naive.py)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Naive KNN (see knn_naive.py)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Complexity\" data-toc-modified-id=\"Time-Complexity-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Time Complexity</a></span></li><li><span><a href=\"#Space-Complexity\" data-toc-modified-id=\"Space-Complexity-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Space Complexity</a></span></li></ul></li><li><span><a href=\"#Quickselect-KNN-(see-knn_quick_select.py)\" data-toc-modified-id=\"Quickselect-KNN-(see-knn_quick_select.py)-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Quickselect KNN (see knn_quick_select.py)</a></span></li><li><span><a href=\"#Binary-Search-Tree-KNN-(see-knn_bst.py)\" data-toc-modified-id=\"Binary-Search-Tree-KNN-(see-knn_bst.py)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Binary Search Tree KNN (see knn_bst.py)</a></span></li></ul></li><li><span><a href=\"#Decision-Boundary\" data-toc-modified-id=\"Decision-Boundary-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Decision Boundary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Voronoi-Diagram\" data-toc-modified-id=\"Voronoi-Diagram-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Voronoi Diagram</a></span></li></ul></li><li><span><a href=\"#Weighted-KNN\" data-toc-modified-id=\"Weighted-KNN-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Weighted KNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Uniform\" data-toc-modified-id=\"Uniform-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Uniform</a></span></li><li><span><a href=\"#Weighted-(inverse)\" data-toc-modified-id=\"Weighted-(inverse)-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Weighted (inverse)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VvTGvHVuzdH"
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Notations and Definitions\n",
    "- Hypothesis Space and the Learning Algorithm\n",
    "- K-Nearest-Neighbours Algorithm\n",
    "  - Time and Space Complexity\n",
    "  - Binary Search Tree\n",
    "  - KD-Tree\n",
    "  - Weighted KNN\n",
    "- Decision Boundary\n",
    "  - Voronoi Diagram\n",
    "- How to choose K\n",
    "  - K = 1\n",
    "  - K = num of sample\n",
    "- Normalization of input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCnAcSz8Ed0w"
   },
   "source": [
    "# Notations and Definitions\n",
    "\n",
    "**Instance Based and Non-Parametric Supervised**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "\n",
    "## Normalization\n",
    "\n",
    "We need to normalize our data before performing KNN. The reasons are as follows, and for visualization see notebook's Normalization Section.\n",
    "\n",
    "Suppose you had a dataset (m \"examples\" by n \"features\") and all but one feature dimension had values strictly between 0 and 1, while a single feature dimension had values that range from -1000000 to 1000000. When taking the euclidean distance between pairs of \"examples\", the values of the feature dimensions that range between 0 and 1 may become uninformative and the algorithm would essentially rely on the single dimension whose values are substantially larger. Just work out some example euclidean distance calculations and you can understand how the scale affects the nearest neighbor computation.\n",
    "\n",
    "From Introduction to Statistical Learning, it is also mentioned that:\n",
    "\n",
    "> Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of 1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of 1,000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1eYYLQWEQe3"
   },
   "source": [
    "# Hypothesis Space and the Learning Algorithm\n",
    "\n",
    "The k-NN regression is a nonparametric approach. The hypothesis space is the class of all functions that are *piecewise constant* on the cells of the $k$th order Voronoi diagram of some set of $n$ points in $\\mathbb R^d$.\n",
    "\n",
    "To be more precise, for a collection of points $\\{x_i\\}_{i=1}^n \\subset \\mathbb R^d$, let $V_j^k(\\{x_i\\}_{i=1}^n)$ be the $j$th Voronoi cell in the $k$th order Voronoi partition of the space by points $\\{x_i\\}_{i=1}^n$ (let us say we order these cells in some way in order to index them). This means that all the points in each of these cells have the same $k$-nearest neighbors among $\\{x_i\\}_{i=1}^n$. Then, the class of functions underlying $k$-NN can be written as\n",
    "\n",
    "$$\n",
    "        \\begin{align}\n",
    "        \\mathcal F_{\\text{$k$-NN}}^{(n)} = \\Big\\{f: \\mathbb R^d \\to \\mathbb R:&\\; \\text{There exists $\\{x_i\\}_{i=1}^n \\subset \\mathbb R^d$ and $\\{a_j\\} \\subset \\mathbb R$ such that} \\\\ &f(x) = \\sum_j a_j \\cdot 1_{V_j^k(\\{x_i\\}_{i=1}^n)}(x), \\quad \\forall x \\in \\mathbb R^d. \\Big\\}.\n",
    "        \\end{align}\n",
    "$$\n",
    "\n",
    "Here, $1_{A}(x) = 1\\{x \\in A\\}$ is the indicator function of set $A$. As you can see, this is a rather large class of functions. There are many ways to pick $n$ points in $\\mathbb R^d$ and each one defines a potentially different Voronoi partition. If you consider all such partitions and all functions that are constant over cells of those partitions, you would get the $k$-NN class.\n",
    "\n",
    "You can also take the union of $F_{\\text{k-NN}}^{(n)}$ over all $n \\in \\{1,2,\\dots\\}$ to define all possible $k$-NN functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqBGXZKNzuVY"
   },
   "source": [
    "# K-Nearest-Neighbours Algorithm\n",
    "\n",
    "## Classification\n",
    "\n",
    "---\n",
    "\n",
    "### Majority Voting Class Label\n",
    "\n",
    "- Calculate the distance from the test query point to all other points in the train dataset\n",
    "\n",
    "- We sort and then store the K nearest neighbours (data points)\n",
    "\n",
    "- Get the majority class from these neighbours\n",
    "\n",
    "- Return this class as the prediction\n",
    "\n",
    "---\n",
    "\n",
    "### Probabilistic Class Label\n",
    "\n",
    "You may have noticed that there is `predict_proba` in the scikit-learn's implementation of k-NN, this comes as no surprise and the implementation is easy. But first let us have some intuition on why the \"soft labels\" are needed. \n",
    "\n",
    "Given our setting of $m$ samples of $\\mathbb{R}^{n}$ dimension, and setting our $k=7 << m$, we consider 2 scenarios with 2 different query point $x_{q1}, x_{q2}$.\n",
    "\n",
    "1. Out of the 7 nearest neighbours of $x_{q1}$, we have 4 positive and 3 negative class labels, by majority voting, we have $x_{q1}$ to be positive, but it is a close one.\n",
    "\n",
    "2. Out of the 7 nearest neighbours of $x_{q2}$, we have all 7 positive class labels, by majority voting, we have $x_{q2}$ to be positive. This is an unanimous decision.\n",
    "\n",
    "Intuitively, we are more confident of $x_{q2}$ to be positive than $x_{q1}$, simply because there are more agreements. It will not be immediately clear if you were to just see the output from the majority class method, as it only tells you whether it is positive or negative. As a result, assigning a probability to the predictions (soft labels) may make more sense and give us a better understanding on which predictions are \"less confident\". \n",
    "\n",
    "Furthermore, if you are computing metrics like ROC, the input must be of soft labels as it is a ranking metric.\n",
    "\n",
    "---\n",
    "\n",
    "## Regression\n",
    "\n",
    "### Majority Mean Class Label\n",
    "\n",
    "- Calculate the distance from the test query point to all other points in the train dataset\n",
    "\n",
    "- We sort and then store the K nearest neighbours (data points)\n",
    "\n",
    "- Get the majority class from these neighbours\n",
    "\n",
    "- Takes the mean of the top K nearest labels and return the mean as the prediction\n",
    "\n",
    "\n",
    "## Additional Implementations\n",
    "\n",
    "### KD-Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlPniG7NzmUN"
   },
   "source": [
    "# Time and Space Complexity\n",
    "\n",
    "## Naive KNN (see knn_naive.py)\n",
    "\n",
    "### Time Complexity\n",
    "\n",
    "First note that Euclidean Distance between two points of dimension $\\mathbb{R}^{n}$ is of $\\mathcal{O}(n)$. This is because the raw form of Euclidean Distance is just one single for loop, performing $n$ subtractions, then $n$ square and lastly $n-1$ additions with one square root operation at the end. This totals up to $(n+n+(n-1)+1)=3n$ which is just $\\mathcal{O}(n)$.\n",
    "\n",
    "Assuming $\\mathcal{X}$ and `y_true` are matrices of $m \\times n$ and $m \\times 1$ dimension respectively.\n",
    "\n",
    "        1. Then the first for loop in the function `KNN_example` is simply $\\mathcal{O}(m \\cdot n)$ because in each loop, we take a total of $\\mathcal{O}(n)$ time as shown earlier in Euclidean Distance, since there is $m$ loops, the total time follows.\n",
    "        2. Sorting the `distance_list` using `sorted` takes $\\mathcal{O}(m \\cdot \\log m)$ - this is a well known time complexity in Python.\n",
    "        3. Calculating the majority class in our code takes $\\mathcal{O}(k)$ time.\n",
    "        4. It totals up to $\\mathcal{O}(m \\cdot n + m \\cdot \\log m + k)$. And since $k$ in our algorithm is usually small relative to $m$ and $n$, we can therefore remove it to become $\\mathcal{O}(m \\cdot n + m \\cdot \\log m)$.\n",
    "        5. Lastly, since we may be feeding in more than one test sample, and assuming there exists $p$ test samples. Then our final time complexity is $\\mathcal{O}(p \\cdot m \\cdot n + p \\cdot m \\cdot \\log m)$.\n",
    "        ---\n",
    "\n",
    "### Space Complexity\n",
    "\n",
    "        It is worth noting that in a general DSA course, we do not consider input as space, but in ML, this may be different. An intuition is if your input data is too memory consuming, your model cannot even train propery. It will throw Memory Limit Exceeded Error!\n",
    "\n",
    "        1. Input Matrix $\\mathcal{X}$ is a list/array which takes up space $\\mathcal{O}(m \\cdot n)$.\n",
    "        2. Distance List `distance_list` takes up roughly $\\mathcal{O}(m \\cdot n)$.\n",
    "        3. Total Space Complexity: $\\mathcal{O}(2 \\cdot m \\cdot n) \\rightarrow \\mathcal{O}(m \\cdot n)$.\n",
    "\n",
    "---\n",
    "\n",
    "## Quickselect KNN (see knn_quick_select.py)\n",
    "\n",
    "Looking at the above code the major bottleneck is the sorting function. We can reduce this by using the quickselect method is which finds the nth smallest element and partitions the array into those smaller than this element and those larger. We can use this method once to get the the k nearest neighbors from a list. The average time complexity of this method is $\\mathcal{O}(m)$ where m is the length of the list on average.\n",
    "\n",
    "        - Time Complexity: ​Since the sorting method becomes $\\mathcal{O}(m)$ then the time complexity become $\\mathcal{O}(m \\times n + m)$ which can be reduced to $\\mathcal{O}(m \\times n)$.\n",
    "\n",
    "        - Space Complexity: Same. We did not change the space complexity since the quickselect method operates directly on our distances list.\n",
    "\n",
    "        See `knn_quick_select.py` for the code.\n",
    "\n",
    "---\n",
    "\n",
    "## Binary Search Tree KNN (see knn_bst.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voronoi Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4YuxJY4vVyj"
   },
   "source": [
    "# Weighted KNN\n",
    "\n",
    "see [here for intuition](https://www.geeksforgeeks.org/weighted-k-nn/)\n",
    "\n",
    "## Uniform\n",
    "\n",
    "The uniform weight says that the distance between a query point $x_{q}$ and any other point $x$ in training set are weighted equally in distance.\n",
    "\n",
    "## Weighted (inverse)\n",
    "\n",
    "Intuitively speaking, if we use 7-NN and out of the 7 neighbours, all 6 of them (all positive class) are super far from $x_{q}$, say hypothetically all within say distance of 0.8-0.9 within $x_{q}$ (normalized inputs assumed), while the last one is near (only negative class), say 0.1 (don't ask why, hypothetical!), then by majority voting, we have to assign it to positive class, but it may not make total sense here to do this, and hence a weighted KNN can be performed, where we assign more weight to the closer points, and much less points to the further points. \n",
    "\n",
    "One popular weighted is using the inverse distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-Yh4mZWwdFj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bugW7ZIswdKe"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

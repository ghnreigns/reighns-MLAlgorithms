{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Intuition\" data-toc-modified-id=\"Intuition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intuition</a></span></li><li><span><a href=\"#Notations-and-Definitions\" data-toc-modified-id=\"Notations-and-Definitions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Notations and Definitions</a></span></li><li><span><a href=\"#Assumptions\" data-toc-modified-id=\"Assumptions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Assumptions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Normalization</a></span></li></ul></li><li><span><a href=\"#Hypothesis-Space-and-the-Learning-Algorithm\" data-toc-modified-id=\"Hypothesis-Space-and-the-Learning-Algorithm-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Hypothesis Space and the Learning Algorithm</a></span></li><li><span><a href=\"#K-Nearest-Neighbours-Algorithm\" data-toc-modified-id=\"K-Nearest-Neighbours-Algorithm-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>K-Nearest-Neighbours Algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Majority-Voting-Class-Label\" data-toc-modified-id=\"Majority-Voting-Class-Label-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Majority Voting Class Label</a></span></li><li><span><a href=\"#Probabilistic-Class-Label\" data-toc-modified-id=\"Probabilistic-Class-Label-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Probabilistic Class Label</a></span></li></ul></li><li><span><a href=\"#Regression\" data-toc-modified-id=\"Regression-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Majority-Mean-Class-Label\" data-toc-modified-id=\"Majority-Mean-Class-Label-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Majority Mean Class Label</a></span></li></ul></li><li><span><a href=\"#Additional-Implementations\" data-toc-modified-id=\"Additional-Implementations-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Additional Implementations</a></span><ul class=\"toc-item\"><li><span><a href=\"#KD-Tree\" data-toc-modified-id=\"KD-Tree-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>KD-Tree</a></span></li></ul></li></ul></li><li><span><a href=\"#Time-and-Space-Complexity\" data-toc-modified-id=\"Time-and-Space-Complexity-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Time and Space Complexity</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naive-KNN-(see-knn_naive.py)\" data-toc-modified-id=\"Naive-KNN-(see-knn_naive.py)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Naive KNN (see knn_naive.py)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Time-Complexity\" data-toc-modified-id=\"Time-Complexity-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Time Complexity</a></span></li><li><span><a href=\"#Space-Complexity\" data-toc-modified-id=\"Space-Complexity-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Space Complexity</a></span></li></ul></li><li><span><a href=\"#Quickselect-KNN-(see-knn_quick_select.py)\" data-toc-modified-id=\"Quickselect-KNN-(see-knn_quick_select.py)-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Quickselect KNN (see knn_quick_select.py)</a></span></li><li><span><a href=\"#Binary-Search-Tree-KNN-(see-knn_bst.py)\" data-toc-modified-id=\"Binary-Search-Tree-KNN-(see-knn_bst.py)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Binary Search Tree KNN (see knn_bst.py)</a></span></li></ul></li><li><span><a href=\"#Decision-Boundary\" data-toc-modified-id=\"Decision-Boundary-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Decision Boundary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Voronoi-Diagram\" data-toc-modified-id=\"Voronoi-Diagram-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Voronoi Diagram</a></span></li></ul></li><li><span><a href=\"#Choose-K\" data-toc-modified-id=\"Choose-K-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Choose K</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-=-1\" data-toc-modified-id=\"K-=-1-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>K = 1</a></span></li><li><span><a href=\"#K-=-m\" data-toc-modified-id=\"K-=-m-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>K = m</a></span></li><li><span><a href=\"#K's-impact-on-Bias-and-Variance\" data-toc-modified-id=\"K's-impact-on-Bias-and-Variance-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>K's impact on Bias and Variance</a></span><ul class=\"toc-item\"><li><span><a href=\"#Small-K\" data-toc-modified-id=\"Small-K-8.3.1\"><span class=\"toc-item-num\">8.3.1&nbsp;&nbsp;</span>Small K</a></span></li><li><span><a href=\"#Big-K\" data-toc-modified-id=\"Big-K-8.3.2\"><span class=\"toc-item-num\">8.3.2&nbsp;&nbsp;</span>Big K</a></span></li></ul></li><li><span><a href=\"#How-to-determine-K?\" data-toc-modified-id=\"How-to-determine-K?-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>How to determine K?</a></span></li></ul></li><li><span><a href=\"#Weighted-KNN\" data-toc-modified-id=\"Weighted-KNN-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Weighted KNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Uniform\" data-toc-modified-id=\"Uniform-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Uniform</a></span></li><li><span><a href=\"#Weighted-(inverse)\" data-toc-modified-id=\"Weighted-(inverse)-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Weighted (inverse)</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Summary</a></span><ul class=\"toc-item\"><li><span><a href=\"#Active-Recall\" data-toc-modified-id=\"Active-Recall-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Active Recall</a></span></li></ul></li></ul></div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Intuition\n",
    "\n",
    "Let us use one of our favourite dataset Iris and just use 2 features, Petal length (pl) and Sepal length (sl). Given a query point $x_{q}$ with only two features, we aim to predict $y_{q}$'s class, whether it is 1, 2 or 3.\n",
    "\n",
    "A decision tree, like its name, is simply asking questions. In a simplified manner, it is merely a yes-no binary question. Programmers can easily relate with a nested `if-else` statement.\n",
    "\n",
    "\n",
    "```python\n",
    "if pl < a: # yes\n",
    "    y = class 1\n",
    "else: # no\n",
    "    if sl < b:\n",
    "        y = class 2\n",
    "    else:\n",
    "        y = class 3\n",
    "```\n",
    "\n",
    "This seems simple, we first ask whether $pl < a$ as the main question, if yes, the our query point is of class 1, if not, we simply check if $sl < b$, if yes, then it is of class 2, and class 3 otherwise.\n",
    "\n",
    "This seems simple, but the keen learner may soon ask: which feature to choose as main node? We will answer this later, but in a nutshell, decision tree does the above, and as we traverse through the nodes, we end up with a final answer (on the class).\n",
    "\n",
    "Note $a, b$ are just cutoff points for the variable $pl, sl$, which denotes in (cm). \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Geometric Intuition\n",
    "\n",
    "Following the image below, we can visualize it in image below. \n",
    "\n",
    "- $\\pi_{1}$: our hyperplane here denotes our first if question, which is our first decision boundary, simply put, left side of $\\pi_{1}$ is the class 1.\n",
    "- $\\pi_{2}$: our hyperplane here denotes our decision on if $sl < b$, and see that anything below this plane is class 2.\n",
    "- $\\pi_{3}$: this follows to be class 3.\n",
    "\n",
    "All hyperplanes are axis-parallel, that is to say, each hyperplane (decision boundary) is parallel to x or y axis (assuming 2 dimensional space) and can be generalized.\n",
    "\n",
    "![title](../data/images/decision_tree_geometric_intuition.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notations and Definitions\n",
    "\n",
    "- root node: First note in a tree.\n",
    "- internal node: Non root and non leaf node, this is where we make decisions.\n",
    "- leaf/terminal note (leaf): End of the node where no decisions are made but points to a class label. \n",
    "\n",
    "## Entropy (Information Theory)\n",
    "\n",
    "Read my notebook named `entropy.ipynb` for more information.\n",
    "\n",
    "## KL Divergence\n",
    "\n",
    "Read my notebook named `kl_divergence.ipynb` for more information.\n",
    "\n",
    "---\n",
    "\n",
    "## Information Gain\n",
    "\n",
    "**Information Gain**: The information gained by splitting the current (sub)-dataset using the attribute.\n",
    "\n",
    "Recall:  Information Gain is a metric that measures the expected reduction in the impurity of the collection $S$, caused by splitting the data according to any given attribute. A chosen attribute $x_i$ divides the example set S into subsets\n",
    "$S_1 , S_2 , ... , S_{C_i}$ according to the $C_i$ distinct values for $x_i$ .\n",
    "The entropy then reduces to the entropy of the subsets $S_1 , S_2 , ... , S_{C_i}$:\n",
    "\n",
    "\n",
    "$$\\text{remainder}(S, x_i) = \\sum_{j=1}^{C_i} \\frac{|S_j|}{|S|} H(S_j)$$\n",
    "\n",
    "\n",
    "The Information Gain (IG; “reduction in entropy”) from knowing the value of $x_i$ is:\n",
    "\n",
    "$$IG(S, x_i) = H(S) - \\text{remainder}(S, x_i) $$  \n",
    "\n",
    "\n",
    "Subsequently, we choose the attribute with the largest IG.\n",
    "\n",
    "---\n",
    "\n",
    "## Gini Impurity\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Assumptions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hypothesis Space and Learning Algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Tree Algorithm\n",
    "\n",
    "## ID3 (Both Classification & Regression)\n",
    "\n",
    "> Invented by Ross Quinlan, ID3 uses a top-down greedy approach to build a decision tree. In simple words, the top-down approach means that we start building the tree from the top and the greedy approach means that at each iteration we select the best feature at the present moment to create a node.\n",
    "\n",
    "### Classification\n",
    "\n",
    "- Given a dataset $\\mathcal{D}$, with a list of tuples $(\\mathrm{x}, \\mathrm{y})$ where we assume that $\\mathrm{y}$ has unique labels `[0,1]`. Note the rest of the notations remain the same, (i.e. m = num_samples)\n",
    "\n",
    "- Given a pre-defined metric $\\mathcal{H}$ where in the case of classification we use Entropy, where for Regression we use MSE. In particular, from this pre-defined metric, we can find the Information Gain of each sub-dataset, we will discuss later, but for now the notation for this is $IG$.\n",
    "  \n",
    "- Calculate the total entropy of $\\mathcal{D}$, more concretely, we do the below:\n",
    "  - Calculate the class frequency for both classes, it should be in the form of $$y^{+} = \\dfrac{\\text{num positives}}{m}$$ $$y^{-} = \\dfrac{\\text{num negatives}}{m}$$     \n",
    "    - If all examples are positive, Return the single-node tree Root, with label = +.\n",
    "    - If all examples are negative, Return the single-node tree Root, with label = -.\n",
    "  - Calculate total entropy of $\\mathcal{D}$ to be $\\mathcal{H}(D)$\n",
    "  \n",
    "-  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pros and Cons\n",
    "\n",
    "## Cons\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "This is well known, if you just use a very naive and plain decision tree, then it is almost guaranteed to not generalize well. A prime example modified from DSFS is as such:\n",
    "> For the sake of example, we are using features of a customer of a bank to predict his credit default rating (discrete ratings). And also just hypothetically say, we are also using a potentially useless feature security social number which is uniquely generated for each customer (note in real world, I do not think we will be feeding this feature in in the first place as in general, we cannot derive useful information from this number). Then further assume, our tree splits to the last node, where the last node is the security number attribute, and then our tree will have $m$ nodes, where $m$ is the number of data (assume distinct customers in dataset). This will cause a problem, because each node has 0 entropy and future predictions will always look into this node of security number (actually if the SSN is unique, then how does tree go down to that unique number for unseen predictions)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2916b9",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307038e3",
   "metadata": {},
   "source": [
    "## Conv Layers\n",
    "\n",
    "- https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a39b55",
   "metadata": {},
   "source": [
    "### Conv1D\n",
    "\n",
    "- https://datascience.stackexchange.com/questions/12830/how-are-1x1-convolutions-the-same-as-a-fully-connected-layer#:~:text=Instead%20of%20a%20single%20output,really%20act%20as%201x1%20convolutions.\n",
    "- https://jdhao.github.io/2017/09/29/1by1-convolution-in-cnn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b02b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- max diff tensor(5.9605e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\r\n",
    "from torch import nn\r\n",
    "\r\n",
    "# Shows that 1x1 conv is similar to a FC layer in terms out final output\r\n",
    "\r\n",
    "m = nn.Conv2d(16, 33, kernel_size=1, stride=1)\r\n",
    "_input = torch.randn(20, 16, 50, 100)\r\n",
    "output1 = m(_input)\r\n",
    "\r\n",
    "fc = nn.Linear(16, 33)\r\n",
    "fc.weight.data = m.weight.squeeze()\r\n",
    "fc.bias.data = m.bias\r\n",
    "output2 = fc(_input.transpose(1, 3)).transpose(1, 3)\r\n",
    "\r\n",
    "print(\"--- max diff\", (output2 - output1).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5796ea1b",
   "metadata": {},
   "source": [
    "### Conv2D\n",
    "\n",
    "- https://stackoverflow.com/questions/55444120/understanding-the-output-shape-of-conv2d-layer-in-keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191fd7a3",
   "metadata": {},
   "source": [
    "### Conv3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d8e46a",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=17MZ4evTef0ypWyALfTaGWj4xgd-BqHOf\" alt=\"parameters\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64009bc6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33a3bf77",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "- https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f295f",
   "metadata": {},
   "source": [
    "## CNN Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493e2bf",
   "metadata": {},
   "source": [
    "### CNN Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6f905",
   "metadata": {
    "tags": []
   },
   "source": [
    "The convolutional and pooling layers in a CNN network are independent of the image size (input shape), this is because the weights of each convolutional layers are calculated only on the number of filters (out_channels). Please refer to my image in Deep Learning Notes on how to calculate number of parameters (which is the number of weights). It is as simple as\n",
    "\n",
    "$$f^{\\ell} \\times f^{\\ell} \\times n_{c}^{\\ell-1} \\times n_{c}^{\\ell} + n_{b}^{\\ell}$$\n",
    "\n",
    "where we denote\n",
    "\n",
    "$f^{\\ell} = \\text{filter size in current layer}$\n",
    "\n",
    "$n_{c}^{\\ell-1} = \\text{number of filters/channels in previous layer}$\n",
    "\n",
    "$n_{c}^{\\ell} = \\text{number of filters in current layer}$\n",
    "\n",
    "$n_{b}^{\\ell} = \\text{number of bias in current layer}$\n",
    "\n",
    "So one can simply calculate the first layer's paramaters/weights as follows given the input size of (3,224,224):\n",
    "\n",
    "$$\\text{number of weights/paramaters} = 3 \\times 3 \\times 3 \\times 16 + 16 = 448$$\n",
    "\n",
    "---\n",
    "\n",
    "and for the second layer it is:\n",
    "\n",
    "$$\\text{number of weights/paramaters} = 5 \\times 5 \\times 16 \\times 32 + 32 = 12832$$\n",
    "\n",
    "---\n",
    "\n",
    "and for the linear layer without bias it is:\n",
    "\n",
    "$$\\text{number of weights/paramaters} = \\text{number of input neurons} \\times \\text{number of output neurons} = 387200 \\times 64 = 24780800$$\n",
    "\n",
    "What I did just now is to make a point that when we calculate the weights/paramaters of each CNN layer, there is absolutely no input shape or image size involved. Thus, the implication is that the number of weights of a CNN layer is **invariant of the input shape**. However, the output shape of each CNN layer is not the same for varying image size, and this will pose a problem - which will be explained in the next part. Before we go, take a moment to run the code below and see that for 2 different input shape 224 vs 448 and you see the only changes are the output shape, the number of weights and parameters are not changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39ac23",
   "metadata": {},
   "source": [
    "However, the issue arises since we are mostly training images using transfer learning. That is to say, we will be using a model that is already trained on a fixed image size. Take ImageNet for example, `VGG16` is trained with 224x224 image sizes. Let us just assume for a moment the model we defined below is `VGG16`, then you should realize that the number of weights and parameters are already fixed once the model is trained. If you **hardcoded** the `Linear()` layer, then you will encounter an error if you run:\n",
    "\n",
    "```python\n",
    "input_image_448 = torch.randn((1, 3, 448, 448))\n",
    "model(input_image_448)\n",
    "```\n",
    "\n",
    "This is because at the `Linear` layer, the model expects an input of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20650ee7",
   "metadata": {},
   "source": [
    "The output of the convolutional layers will have different spatial sizes for differently sized images, and this will cause an issue if we have a fully connected layer afterwards (since our fully connected layer requires a fixed size input). So for example **VGG16** which is pretrained on `imagenet` with image sizes of 224x224, then when you load the `state dict`, the number of weights and parameters are already fixed. To be more verbose, the number of weights for the convolutional layers stay the same for any input image size, but the fully connected layers will not. For example, in the native resolution of 224x224, the layer before the fully connected layer is a convolutional layer and subsequent pooling layer - which has an output shape of `(-1, 512, 7, 7)`. We need to flatten this pooling layer into a dense layer first, one can imagine in a 3-dimensional perpective that we squashed a pool of 3-dimensional neurons into a vertical fully connected neurons. Refer to this image: \n",
    "\n",
    "Now, some intuition needs to be provided here, for the absent minded (me), look further after the image pasted above for the math behind weights (pages after). \n",
    "\n",
    "Continuing above, we know that the learnable weights of the flattened layer is $512\\times 7\\times 7 = 25088$, and since we are connected to a pre-defined fully connected layer of 4096 neurons, then it follows that in this very fully connected layer, we will output $$25088\\times 4096 + 4096 = 102764544$$ weights. This is a fixed number and will change if you change the image input size.\n",
    "\n",
    "For example, if I were to input a 512x512 image, see the code above, then the previous layer before the fully connected layer is actually `[-1, 512, 16, 16]` which is not the same as $512\\times 7 \\times 7$. This will lead to our weights mismatched at the fully connected layer. So we can solve this problem by using `nn.AdaptiveAvgPooling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef15aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary \r\n",
    "import torch\r\n",
    "from torch import nn\r\n",
    "from prettytable import PrettyTable\r\n",
    "\r\n",
    "def get_output_shape(model, image_dim):\r\n",
    "    return model(torch.rand(*(image_dim))).data.shape\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "def count_parameters(model):\r\n",
    "    \"\"\"Counts the number of learnable parameters in each layer of a PyTorch Model.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        model ([type]): [description]\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        [type]: [description]\r\n",
    "    \"\"\"\r\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\r\n",
    "    total_params = 0\r\n",
    "    for name, parameter in model.named_parameters():\r\n",
    "        if not parameter.requires_grad:\r\n",
    "            continue\r\n",
    "        param = parameter.numel()\r\n",
    "        table.add_row([name, param])\r\n",
    "        total_params += param\r\n",
    "    print(table)\r\n",
    "    print(f\"Total Trainable Params: {total_params}\")\r\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4373135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 224, 224])\n",
      "torch.Size([1, 16, 224, 224])\n",
      "torch.Size([1, 16, 112, 112])\n",
      "torch.Size([1, 32, 110, 110])\n",
      "torch.Size([1, 32, 110, 110])\n",
      "torch.Size([1, 387200])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0094,  0.0795, -0.0394,  0.0066,  0.0128, -0.1164,  0.0741,  0.1371,\n",
       "          0.1820, -0.0125]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Model, self).__init__()\r\n",
    "        self.net = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False),\r\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(5,5), stride=(1,1), padding=(1,1)),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Flatten(),\r\n",
    "            nn.Linear(387200, 64),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(64, 10),\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        for layer in self.net:\r\n",
    "            x = layer(x)\r\n",
    "            print(x.size())\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "model = Model()\r\n",
    "input_image_224 = torch.randn((1, 3, 224, 224))\r\n",
    "input_image_448 = torch.randn((1, 3, 448, 448))\r\n",
    "\r\n",
    "# Let's prin) it\r\n",
    "model(input_image_224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb5d40be-a06a-45f4-bcb3-9859aca571c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|   Modules    | Parameters |\n",
      "+--------------+------------+\n",
      "| net.0.weight |    432     |\n",
      "|  net.0.bias  |     16     |\n",
      "| net.3.weight |   12800    |\n",
      "|  net.3.bias  |     32     |\n",
      "| net.6.weight |  24780800  |\n",
      "|  net.6.bias  |     64     |\n",
      "| net.8.weight |    640     |\n",
      "|  net.8.bias  |     10     |\n",
      "+--------------+------------+\n",
      "Total Trainable Params: 24794794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24794794"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdf9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to('cuda')\n",
    "# model_summary = torchsummary.summary(model, (3,224,224))\n",
    "# model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70654562",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(input_image_448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee015e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|   Modules    | Parameters |\n",
      "+--------------+------------+\n",
      "| net.0.weight |    432     |\n",
      "|  net.0.bias  |     16     |\n",
      "| net.3.weight |   12800    |\n",
      "|  net.3.bias  |     32     |\n",
      "| net.6.weight |  24780800  |\n",
      "|  net.6.bias  |     64     |\n",
      "| net.8.weight |    640     |\n",
      "|  net.8.bias  |     10     |\n",
      "+--------------+------------+\n",
      "Total Trainable Params: 24794794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24794794"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c607ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(Model, self).__init__()\r\n",
    "        self.net = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1)),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.MaxPool2d(kernel_size=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False),\r\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(5,5), stride=(1,1), padding=(1,1)),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.AdaptiveAvgPool2d(110,110),\r\n",
    "            nn.Flatten(),\r\n",
    "            nn.Linear(387200, 64),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(64, 10),\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        for layer in self.net:\r\n",
    "            x = layer(x)\r\n",
    "            print(x.size())\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4dbe1",
   "metadata": {},
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73766c31",
   "metadata": {},
   "source": [
    "### AdaptiveAvgPool\n",
    "\n",
    "Mostly used in CNN models to make input image size invariant.\n",
    "\n",
    "- https://www.zhihu.com/question/282046628\n",
    "- https://stackoverflow.com/questions/58692476/what-is-adaptive-average-pooling-and-how-does-it-work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7c6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# target output size of 7x7 (square)\r\n",
    "m = nn.AdaptiveAvgPool2d(7)\r\n",
    "input = torch.randn(1, 64, 32, 32)\r\n",
    "output = m(input)\r\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60a592695384e68e81c96d019436b4c6d949e7549cb1292bfa72c05e2c5f0fcb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

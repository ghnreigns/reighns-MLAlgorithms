{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9598cbd0",
   "metadata": {},
   "source": [
    "# Intuition\n",
    "\n",
    "\n",
    "Let us talk this in a less formal way without invoking formal definitions of \"probability distributions\".\n",
    "\n",
    "## Train and Test Distribution\n",
    "\n",
    "Let us say we are imposed a Machine Learning problem to predict cats vs dogs. We are handed a training set, and there is also a hidden test set that we do not know of. Let us pretend we browse through each image in the training set $\\mathcal{D}$, and plot out the pixel distribution of each image, we observe that each image pixel distribution follows a gaussian curve with mean 0 and std 1 (i.e. each image in $\\mathcal{D}$ is generated from $\\mathcal{N}(0,1)$). Then we trained a good model using training set, but performed badly on the test set $\\mathcal{D}_{test}$, and after we also plot the pixel density of the test set, we discovered that this set follows a very different distributions, say follow $\\mathcal{N}(255, 255)$, and each image has 10 times the resolution of the training set. We can therefore conclude, that, both train and test set have different \"probability distributions\".\n",
    "\n",
    "---\n",
    "\n",
    "## Distribution from your model\n",
    "\n",
    "Let us also continue our example from the cat vs dog, this time we assume both train, test and every other images for dogs and cats are generated from $\\mathcal{N}(0, 1)$. That is to say, we **know** our population distribution, and if we do know, then why do we need a Machine Learning model at all? We can just approximate all images with the known population distribution with the parameters. Even simpler, if we have a dataset that is beknownst to us with a perfect linear relationship that can be predicted perfectly by a simple linear regression model, (i.e. the population parameter is known), then why do we even need linear regression?\n",
    "\n",
    "However, this world is far from perfect, and we do not know the population parameters of the underlying distribution of the dataset given to us, hence we use models to predict it.\n",
    "\n",
    "Thus, we can analogously say that if a training set $\\mathcal{D}$ follows a distribution $p$, then our model follows a distribution $q$, where in the simplest case in linear regression, we know our $q$ follows a distribution with 2 parameters, $m$ the slope, $c$ the bias.\n",
    "\n",
    "With that in mind, our machine learning models can be phrased as using $q$ to approximate $q$. And this is often used in Cross-Entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a92881",
   "metadata": {},
   "source": [
    "[kdnuggets-train-test-distribution](https://www.kdnuggets.com/2019/01/when-your-training-testing-data-different-distributions.html)\n",
    "[analytics-vidhya-entropy-loss](https://medium.com/analytics-vidhya/understanding-entropy-the-golden-measurement-of-machine-learning-4ea97c663dc3#:~:text=By%20using%20entropy%20in%20machine,be%20desired%20in%20model%2Dbuilding.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
